# -*- coding: utf-8 -*-
"""doc_retrieve.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jAIAQ3aPu5RyBUi_zUhiI9Jaipaobdig
"""

!pip install -q pdfplumber pymupdf pillow chromadb sentence-transformers transformers playwright requests nest_asyncio

# Setup Playwright
!playwright install chromium

import textwrap
import pdfplumber
import fitz
import chromadb
import requests
import io
import asyncio
import nest_asyncio
from PIL import Image
from sentence_transformers import SentenceTransformer
from playwright.async_api import async_playwright
from transformers import pipeline
from IPython.display import display, Markdown
from google.colab import files

# Initialize async environment
nest_asyncio.apply()

# Initialize models
text_encoder = SentenceTransformer('all-MiniLM-L6-v2')
llm = pipeline("text-generation", model="gpt2", max_length=1024)

async def web_screenshot(url):
    """Async webpage screenshot capture"""
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            await page.goto(url, timeout=30000)
            screenshot = await page.screenshot()
            await browser.close()
        return screenshot
    except Exception as e:
        print(f"Web capture error: {e}")
        return None

def preprocess_pdf(pdf_path):
    """Process PDF with paragraph-level chunking"""
    client = chromadb.PersistentClient(path="./chroma_db")
    collection = client.get_or_create_collection("visual_rag")

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                paragraphs = [p.strip() for p in text.split('\n') if p.strip()]

                for para_num, para in enumerate(paragraphs):
                    words = page.extract_words()
                    para_words = [w for w in words if w['text'] in para]

                    if para_words:
                        x0 = min(w['x0'] for w in para_words)
                        top = min(w['top'] for w in para_words)
                        x1 = max(w['x1'] for w in para_words)
                        bottom = max(w['bottom'] for w in para_words)
                        bbox = (x0, top, x1, bottom)

                        collection.add(
                            ids=f"p{page_num}_para{para_num}",
                            documents=para,
                            metadatas={
                                "page": page_num,
                                "bbox": str(bbox),
                                "type": "text"
                            }
                        )
    print(f"‚úÖ PDF processed: {pdf_path}")

class VisualRAG:
    def __init__(self, pdf_path):
        self.pdf_path = pdf_path
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.client.get_collection("visual_rag")

    def _get_para_screenshot(self, page_num, bbox_str):
        """Generate paragraph screenshot from PDF"""
        doc = fitz.open(self.pdf_path)
        page = doc.load_page(page_num)
        bbox = tuple(map(float, bbox_str.strip('()').split(',')))
        return page.get_pixmap(clip=bbox).tobytes()

    def _truncate_context(self, text, max_tokens=900):
        """Ensure context fits model limits"""
        return ' '.join(text.split()[:max_tokens])

    async def query(self, question, api_key):
        """Full query pipeline with async web handling"""
        # PDF Retrieval
        results = self.collection.query(
            query_texts=[question],
            n_results=3,
            include=["documents", "metadatas"]
        )

        # Process PDF sources
        pdf_sources = []
        pdf_context = []
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            truncated = self._truncate_context(doc)
            pdf_context.append(f"[PDF] {truncated}")
            pdf_sources.append({
                "type": "pdf",
                "page": meta["page"],
                "text": doc,
                "screenshot": self._get_para_screenshot(meta["page"], meta["bbox"])
            })

        # Web Search
        web_sources = []
        web_context = []
        try:
            web_resp = requests.get(
                "https://serpapi.com/search",
                params={
                    "q": question,
                    "api_key": api_key,
                    "engine": "google",
                    "num": 2
                },
                timeout=15
            )

            if web_resp.status_code == 200:
                web_results = web_resp.json().get("organic_results", [])[:2]
                for result in web_results:
                    if 'snippet' in result:
                        web_context.append(f"[Web] {self._truncate_context(result['snippet'])}")
                        web_sources.append({
                            "type": "web",
                            "url": result['link'],
                            "snippet": result['snippet']
                        })
        except Exception as e:
            print(f"Web search error: {e}")

        # Generate answer
        context = "\n".join(pdf_context + web_context)
        prompt = f"""Answer the question using these sources. Cite as [PDF] or [Web].
        Question: {question}
        Context: {context}
        Answer:"""

        answer = llm(prompt, max_length=1024, num_return_sequences=1)[0]['generated_text']
        answer = answer.split("Answer:")[-1].strip()

        # Capture web screenshots async
        for source in web_sources:
            source['screenshot'] = await web_screenshot(source['url'])

        return answer, pdf_sources + web_sources

async def main():
    # Upload PDF
    print("üì§ Upload PDF file:")
    uploaded = files.upload()
    pdf_name = next(iter(uploaded))
    preprocess_pdf(pdf_name)

    # Initialize RAG
    rag = VisualRAG(pdf_name)
    api_key = input("üîë Enter SerpAPI key: ")

    # Query loop
    while True:
        print("\n" + "="*50)
        question = input("‚ùì Ask a question (q to quit): ")
        if question.lower() == 'q':
            break

        answer, sources = await rag.query(question, api_key)

        # Display results
        print("\nüí° Answer:")
        display(Markdown(answer))

        print("\nüîç Sources:")
        for i, source in enumerate(sources, 1):
            print(f"\nüìå [Source {i}] {source['type'].upper()}")
            if source["type"] == "pdf":
                print(f"Page {source['page']+1}:")
                display(Image.open(io.BytesIO(source['screenshot'])))
                print(textwrap.shorten(source['text'], width=200))
            else:
                print(f"URL: {source['url']}")
                if 'snippet' in source:
                    print(textwrap.shorten(source['snippet'], width=200))
                if source.get('screenshot'):
                    display(Image.open(io.BytesIO(source['screenshot'])))

# %% [code]
if __name__ == "__main__":
    asyncio.run(main())















